{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Location via Indoor Positioning System \n",
    "## Analysis and Modeling in Python *prototypes and drabbles\n",
    "This script and subsequent notebooks are the Python conversion of the Indoor Location case study from the book, 'Data Science in R' by Nolan and Lang. http://rdatasciencecases.org/code.html <BR>\n",
    "* Team: Jonathan Tan, Lance Dacy, Reannan Mcdaniel, Shawn Jung \n",
    "* Last Update: 5/14/2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from __future__ import print_function\n",
    "#from statsmodels.distributions.empirical_distribution import ECDF\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load dataframes\n",
    "#offline_pivot = pd.read_csv('./data/offline_pivot.csv')\n",
    "#online_pivot = pd.read_csv('./data/online_pivot.csv')\n",
    "offline_pivot = pd.read_csv('D:/Code/QTW Case Study 1/offline_pivot.csv')\n",
    "online_pivot = pd.read_csv('D:/Code/QTW Case Study 1/online_pivot.csv')\n",
    "\n",
    "# re-add pos_x and pos_y for distance error calculation in later stage\n",
    "offline_pivot[['pos_x', 'pos_y']] = offline_pivot['pos_xy'].str.split('-',n=2, expand=True).astype(float)\n",
    "online_pivot[['pos_x', 'pos_y']] = online_pivot['pos_xy'].str.split('-',n=2, expand=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = online_pivot.iloc[0,1:8]\n",
    "train0 = offline_pivot.iloc[0,1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#offline_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 166 entries, 0 to 165\nData columns (total 10 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   pos_xy             166 non-null    object \n 1   00:0f:a3:39:dd:cd  166 non-null    float64\n 2   00:0f:a3:39:e1:c0  166 non-null    float64\n 3   00:14:bf:3b:c7:c6  166 non-null    float64\n 4   00:14:bf:b1:97:81  166 non-null    float64\n 5   00:14:bf:b1:97:8a  166 non-null    float64\n 6   00:14:bf:b1:97:8d  166 non-null    float64\n 7   00:14:bf:b1:97:90  166 non-null    float64\n 8   pos_x              166 non-null    float64\n 9   pos_y              166 non-null    float64\ndtypes: float64(9), object(1)\nmemory usage: 12.4+ KB\n"
    }
   ],
   "source": [
    "offline_pivot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "average Accuracy For pos_X=  22.316176470588232 +/- 7.885236659728786\naverage Accuracy For pos_Y=  44.338235294117645 +/- 16.601371519611863\n"
    }
   ],
   "source": [
    "#RF test\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#single class prediction on pos_x, then pos_y??\n",
    "y = offline_pivot.iloc[:, 8] \n",
    "x = offline_pivot.iloc[:, 1:8]\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10)\n",
    "#significant of n_estimators? number of trees?\n",
    "clf = RandomForestClassifier(n_estimators= 100, random_state = 1)\n",
    "acc = cross_val_score(clf, x, y = y, cv = cv)\n",
    "\n",
    "print(\"average Accuracy For pos_X= \", acc.mean()*100, \"+/-\", acc.std()*100)\n",
    "#confusion matrix for mean accuracy?\n",
    "\n",
    "y = offline_pivot.iloc[:, 9] #for pos_y\n",
    "x = offline_pivot.iloc[:, 1:8]\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 100, random_state = 1)\n",
    "acc = cross_val_score(clf, x, y = y, cv = cv)\n",
    "\n",
    "print(\"average Accuracy For pos_Y= \", acc.mean()*100, \"+/-\", acc.std()*100)\n",
    "#hmm not great, 22% acc for x, 44% acc for y for Random Forest Classifier with 100 \"branches\" and 10 splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single class prediction on pos_x, then pos_y?? \n",
    "#   #   #   #\n",
    "#doesn't work\n",
    "#   #   #   #\n",
    "#y = offline_pivot.iloc[:, 0].to_frame() #use combined position column as predictor\n",
    "#x = offline_pivot.iloc[:, 1:8]\n",
    "\n",
    "#cv = StratifiedKFold(n_splits = 2)\n",
    "#significant of n_estimators? number of trees?\n",
    "#clf = RandomForestClassifier(n_estimators= 100, random_state = 1)\n",
    "#acc = cross_val_score(clf, x, y = y, cv = cv)\n",
    "\n",
    "#print(\"average Accuracy\", acc.mean()*100, \"+/-\", acc.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m3mbers in each class\n",
    "#from collections import Counter\n",
    "#Counter(offline_pivot.iloc[:, 0]).keys()\n",
    "#Counter(offline_pivot.iloc[:, 0]).values()\n",
    "#so using the combined (x, y) coords looks like they are all unique? not suitable for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to visualize results from this format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "#knn split class prediction \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = offline_pivot.iloc[:, 8] #just on pos_x \n",
    "x = offline_pivot.iloc[:, 1:8]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .2, random_state = 83812)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors = 2)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "A = knn.kneighbors_graph(x)\n",
    "A.toarray()\n",
    "#need to fit some kind of metric to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "KMeans Pos_X accuracy  = 42.17%\nKMeans Pos_Y accuracy  = 31.33%\n"
    }
   ],
   "source": [
    "#kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "x2 = offline_pivot.iloc[:, 1:8] #fit kmeans to predictors, 7 macs\n",
    "y2 =offline_pivot.iloc[:, 9:10]\n",
    "\n",
    "cls = KMeans(n_clusters = 7, init = 'k-means++', random_state = 1)\n",
    "cls.fit(x2)\n",
    "newfeature = cls.labels_\n",
    "\n",
    "X = offline_pivot.iloc[:, 1:8]\n",
    "X = np.column_stack((X,pd.get_dummies(newfeature))) #use for comparison with acc cross val\n",
    "\n",
    "acc = cross_val_score(clf, X, y = y2, cv = cv)\n",
    "print('KMeans Pos_X accuracy  = {:.2f}%'.format(acc[0]*100))\n",
    "print('KMeans Pos_Y accuracy  = {:.2f}%'.format(acc[1]*100))\n",
    "#not better than random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    https://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Hamming Score: 0.8823529411764706\n"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = offline_pivot.iloc[:, 8:10] \n",
    "x = offline_pivot.iloc[:, 1:8]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .2, random_state = 83812)\n",
    "\n",
    "classifier = BinaryRelevance(GaussianNB()) #binary relevance multiclassifier \n",
    "#knn = KNeighborsClassifier(n_neighbors = 4)\n",
    "#classifier = MultiOutputClassifier(knn, n_jobs = -1)\n",
    "classifier.fit(x_train, y_train) #fit\n",
    "predictions = classifier.predict(x_test) #predict\n",
    "\n",
    "#both to array\n",
    "y_test = y_test.values #convert dataframe to array #so predictions from knn come out as array, but accuracy score doesn't support multiclass?\n",
    "predictions = predictions.toarray()\n",
    "\n",
    "#try converting both to sparse? \n",
    "#y_test_sparse = sparse.csc_matrix(y_test) #convert y_test t0 sparse? csc\n",
    "#predictions_sparse = sparse.csc_matrix(predictions) \n",
    "\n",
    "#try matrix conversion?\n",
    "#y_test = np.matrix(y_test)\n",
    "#predictions = np.matrix(predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#accuracy_score(y_test, predictions)\n",
    "#accuracy_score(y_test_sparse, predictions_sparse) ok both sparse to accuracy_score gives a len() error?\n",
    "\n",
    "print('Hamming Score:', hamming_score(y_test, predictions)) #Hamming score is the difference in positions?\n",
    "#print('Subset Accuracy:', sklearn.metrics.accuracy_score(y_test, predictions))\n",
    "#print('Hamming Loss:', sklearn.metrics.hamming_loss(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamming Distance Example\n",
    "\n",
    "karolin\n",
    "kathrin\n",
    "  |||      Different at 3 positions so a distance of 3\n",
    "\n",
    "1011101\n",
    "1001001\n",
    "  | |      Distance of 2\n",
    "\n",
    "2173896\n",
    "2233796\n",
    " || |      Distance of 3\n",
    "\n",
    "\n",
    "how to reconcile accuracy with hamming score? how to get accuracy out of multiclass/multilabel output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiclass multivariable output RF\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "y = offline_pivot.iloc[:, 8:10] \n",
    "x = offline_pivot.iloc[:, 1:8]\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 10)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 100, random_state = 1)\n",
    "#acc = cross_val_score(clf, x, y = y, cv = cv)\n",
    "#multiclass, multioutput not compatible with cross val score\n",
    "#print(\"average Accuracy = \", acc.mean()*100, \"+/-\", acc.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(test_array, train_df, number_of_returns=1):\n",
    "    dist = []\n",
    "    for i in range(train_df.shape[0]):\n",
    "        d = np.sqrt(np.sum(np.square((np.array(test_array) - np.array(train_df)[i]))))\n",
    "        dist.append(d)\n",
    "    \n",
    "    dist_result = np.array(dist)\n",
    "    \n",
    "    # return top n lowest values\n",
    "\n",
    "    return dist_result.argsort()[0:number_of_returns][::-1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "predicted location: 18    1.0-2.0\nName: pos_xy, dtype: object\noriginal location: 0.0-0.05\n"
    }
   ],
   "source": [
    "test0_pred = distance(test0, offline_pivot.iloc[:,1:8], 1)\n",
    "\n",
    "print('predicted location:', offline_pivot.iloc[test0_pred]['pos_xy'])\n",
    "print('original location:', online_pivot.iloc[0]['pos_xy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38232bit10390a088e364bd1a18b69aa0008d7e8",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}